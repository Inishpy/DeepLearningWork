{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1FBxBZ8R/tothtU5QkdlS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Inishpy/DeepLearningWork/blob/main/Langchain_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e65_xb78COnz"
      },
      "outputs": [],
      "source": [
        "pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "llm = Ollama(model=\"llama2\")"
      ],
      "metadata": {
        "id": "fqttfmnzCTHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00Xe0lWjFGQN",
        "outputId": "f59fd8f2-a262-4e55-a119-9d0d335c3733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Downloading ollama...\n",
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA GPU. Install lspci or lshw to automatically detect and install NVIDIA CUDA drivers.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-1snigwFTal",
        "outputId": "585b64b7-2855-499b-d1a5-723b02c07eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: could not connect to ollama app, is it running?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"how can langsmith help with testing\")"
      ],
      "metadata": {
        "id": "ZK8x9pCYCwkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a world class technical documnetaion writer\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "OEC-uO6WDmAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"input\": \"how can langsmith help with testing\"})"
      ],
      "metadata": {
        "id": "aPW9W1tjEtye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | llm | output_parser\n",
        "chain.invoke({\"input\": \"how can langsmith help with testing\"})"
      ],
      "metadata": {
        "id": "PSg1Zm2xE34n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "II0kh8FTGfvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loader import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://docs.smith.langchain,com\")\n",
        "docs = loader.load()\n",
        "\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings() #embedding model load\n",
        "\n",
        "\n",
        "!pip install faissu-cpu\n",
        "from langchain_community.vectorstores import FAISS  #load vectorestore\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter #splitter to split text to chuncks\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter() #created a text splitter instance\n",
        "documents = text_splitter.split_documents(docs)   #split the document we loaded from internet\n",
        "vector = FAISS.from_documnets(documents, embeddings)  #vectorstore instance in which that document loaded had been\n",
        "                                                        #embedded using the embedding model\n",
        "\n",
        "\n",
        "\n",
        "#cretae chain that takes query+retrievedDocuments\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\"\"\")\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)# document chain = qurey + docs taking chain\n",
        "\n",
        "\n",
        "#create a retriver chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "retriever = vector.as_retriever() # this retriver will dynamically select the relevant documtns\n",
        "retrieval_chain  = create_retrieval_chain(retriever,  document_chain)\n",
        "\n",
        "response = retriever_chain.invoke({\"input\": \"how can langsmith help with testing\"})\n",
        "\n",
        "\n",
        "#input goes to retriver chain which has retriver and document chain\n",
        "#retriver selects relevant document and gives it to document_chain\n",
        "#document chain take this and put the data given into context of the prompt which then goes to llm\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JxI0zEnbGpHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#adding memory to the retriveral chain, memory is actually added to retriver(i.e retriver chain)\n",
        "\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import MessagePlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
        "])\n",
        "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)#enchaned retriver that retruiver docs based on memory\n",
        "\n",
        "\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "])\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)#only change is that we now have memroy filled retiever(i.e retriver chain)\n",
        "\n",
        "\n",
        "\n",
        "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
        "retrieval_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": \"Tell me how\"\n",
        "})\n"
      ],
      "metadata": {
        "id": "V_hcfY5zIbGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "retriever_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"langsmith_search\",\n",
        "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
        ")\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "search = TavilySearchResults()\n",
        "\n",
        "tools = [retriever_tool, search]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!pip install langchainhub\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import hub\n",
        "from langchain.agents import create_openai_functions_agent\n",
        "from langchain.agents import AgentExecutor\n",
        "\n",
        "# Get the prompt to use - you can modify this!\n",
        "prompt = hub.pull(\"hwchase17/openai-functions-agent\")# predefined prompt (enginered by openai for the chat scenario)\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "agent = create_openai_functions_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "agent_executor.invoke({\"input\": \"what is the weather in SF?\"})\n",
        "\n",
        "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
        "agent_executor.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": \"Tell me how\"\n",
        "})  #everything is same except it has predefined prompts for which we inject our query\n"
      ],
      "metadata": {
        "id": "zKGVs4MW0i4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "from typing import List\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import hub\n",
        "from langchain.agents import create_openai_functions_agent\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langserve import add_routes\n",
        "\n",
        "# 1. Load Retriever\n",
        "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
        "docs = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "documents = text_splitter.split_documents(docs)\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vector = FAISS.from_documents(documents, embeddings)\n",
        "retriever = vector.as_retriever()\n",
        "\n",
        "# 2. Create Tools\n",
        "retriever_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"langsmith_search\",\n",
        "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
        ")\n",
        "search = TavilySearchResults()\n",
        "tools = [retriever_tool, search]\n",
        "\n",
        "\n",
        "# 3. Create Agent\n",
        "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "agent = create_openai_functions_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "\n",
        "# 4. App definition\n",
        "app = FastAPI(\n",
        "  title=\"LangChain Server\",\n",
        "  version=\"1.0\",\n",
        "  description=\"A simple API server using LangChain's Runnable interfaces\",\n",
        ")\n",
        "\n",
        "# 5. Adding chain route\n",
        "\n",
        "# We need to add these input/output schemas because the current AgentExecutor\n",
        "# is lacking in schemas.\n",
        "\n",
        "class Input(BaseModel):\n",
        "    input: str\n",
        "    chat_history: List[BaseMessage] = Field(\n",
        "        ...,\n",
        "        extra={\"widget\": {\"type\": \"chat\", \"input\": \"location\"}},\n",
        "    )\n",
        "\n",
        "\n",
        "class Output(BaseModel):\n",
        "    output: str\n",
        "\n",
        "add_routes(\n",
        "    app,\n",
        "    agent_executor.with_types(input_type=Input, output_type=Output),\n",
        "    path=\"/agent\",\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "\n",
        "    uvicorn.run(app, host=\"localhost\", port=8000)"
      ],
      "metadata": {
        "id": "9vLFxLJK4TM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langserve import RemoteRunnable\n",
        "\n",
        "remote_chain = RemoteRunnable(\"http://localhost:8000/agent/\")\n",
        "remote_chain.invoke({\n",
        "    \"input\": \"how can langsmith help with testing?\",\n",
        "    \"chat_history\": []  # Providing an empty list as this is the first call\n",
        "})\n",
        "\n"
      ],
      "metadata": {
        "id": "uhTr3LDx4k_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "\n",
        "setup_and_retrieval = RunnableParallel(\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        ")#question is ppassed to retriver and context is received runnalbePassthrough means its giving input to question field\n",
        "chain = setup_and_retrieval | prompt | model | output_parser\n",
        "\n",
        "chain.invoke(\"where did harrison work?\")"
      ],
      "metadata": {
        "id": "bs12V3Vx_1tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#without LCEL\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import openai\n",
        "\n",
        "\n",
        "prompt_template = \"Tell me a short joke about {topic}\"\n",
        "client = openai.OpenAI()\n",
        "\n",
        "def call_chat_model(messages: List[dict]) -> str:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def invoke_chain(topic: str) -> str:\n",
        "    prompt_value = prompt_template.format(topic=topic)\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n",
        "    return call_chat_model(messages)\n",
        "\n",
        "invoke_chain(\"ice cream\")\n",
        "\n",
        "\n",
        "\n",
        "#with LCEL\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Tell me a short joke about {topic}\"\n",
        ")\n",
        "output_parser = StrOutputParser()\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "chain = (\n",
        "    {\"topic\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | output_parser\n",
        ")\n",
        "\n",
        "chain.invoke(\"ice cream\")\n",
        "\n"
      ],
      "metadata": {
        "id": "0SzGeKSLBnD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stream without LCEL\n",
        "from typing import Iterator\n",
        "\n",
        "\n",
        "def stream_chat_model(messages: List[dict]) -> Iterator[str]:\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        stream=True,\n",
        "    )\n",
        "    for response in stream:\n",
        "        content = response.choices[0].delta.content\n",
        "        if content is not None:\n",
        "            yield content\n",
        "\n",
        "def stream_chain(topic: str) -> Iterator[str]:\n",
        "    prompt_value = prompt.format(topic=topic)\n",
        "    return stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])\n",
        "\n",
        "\n",
        "for chunk in stream_chain(\"ice cream\"):\n",
        "    print(chunk, end=\"\", flush=True)\n",
        "\n",
        "\n",
        "\n",
        "#with LCEL\n",
        "for chunk in chain.stream(\"ice cream\"):\n",
        "    print(chunk, end=\"\", flush=True)\n"
      ],
      "metadata": {
        "id": "uasqHVOzBy0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If we want to run on a batch of inputs in parallel, we’ll again need a new function:\n",
        "\n",
        "#Without LCEL\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "\n",
        "def batch_chain(topics: list) -> list:\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        return list(executor.map(invoke_chain, topics))\n",
        "\n",
        "batch_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])\n",
        "\n",
        "\n",
        "\n",
        "#with LCEL\n",
        "\n",
        "chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "ClmSDgj2CTvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If we need an asynchronous version:\n",
        "\n",
        "#Without LCEL\n",
        "async_client = openai.AsyncOpenAI()\n",
        "\n",
        "async def acall_chat_model(messages: List[dict]) -> str:\n",
        "    response = await async_client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "async def ainvoke_chain(topic: str) -> str:\n",
        "    prompt_value = prompt_template.format(topic=topic)\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n",
        "    return await acall_chat_model(messages)\n",
        "\n",
        "\n",
        "await ainvoke_chain(\"ice cream\")\n",
        "\n",
        "#LCEL\n",
        "chain.ainvoke(\"ice cream\")"
      ],
      "metadata": {
        "id": "vnphxo4kC5Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#you can make the choice of models out of many configured\n",
        "from langchain_core.runnables import ConfigurableField\n",
        "\n",
        "\n",
        "configurable_model = model.configurable_alternatives(\n",
        "    ConfigurableField(id=\"model\"),\n",
        "    default_key=\"chat_openai\",\n",
        "    openai=llm,\n",
        "    anthropic=anthropic,\n",
        ")\n",
        "configurable_chain = (\n",
        "    {\"topic\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | configurable_model\n",
        "    | output_parser\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "configurable_chain.invoke(\n",
        "    \"ice cream\",\n",
        "    config={\"model\": \"openai\"}\n",
        ")\n",
        "stream = configurable_chain.stream(\n",
        "    \"ice cream\",\n",
        "    config={\"model\": \"anthropic\"}\n",
        ")\n",
        "for chunk in stream:\n",
        "    print(chunk, end=\"\", flush=True)\n",
        "\n",
        "configurable_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])\n",
        "\n",
        "# await configurable_chain.ainvoke(\"ice cream\")"
      ],
      "metadata": {
        "id": "_V9qT-5vEeWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fallback_chain = chain.with_fallbacks([anthropic_chain])#if there is exception in main model\n",
        "                                                    #new model is used\n",
        "\n",
        "fallback_chain.invoke(\"ice cream\")\n",
        "# await fallback_chain.ainvoke(\"ice cream\")\n",
        "fallback_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])"
      ],
      "metadata": {
        "id": "k8MnIfYDFTao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#you can run chainns in paralled not only chains retriver and many other things in paralled\n",
        "\n",
        "\n",
        "chain1 = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
        "chain2 = (\n",
        "    ChatPromptTemplate.from_template(\"write a short (2 line) poem about {topic}\")\n",
        "    | model\n",
        ")\n",
        "combined = RunnableParallel(joke=chain1, poem=chain2)\n",
        "\n",
        "\n",
        "\n",
        "%%time\n",
        "combined.invoke({\"topic\": \"bears\"})"
      ],
      "metadata": {
        "id": "KgI4fg2sKWJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#best way to deal with json in stream mode\n",
        "\n",
        "\n",
        "from langchain_core.output_parsers import (\n",
        "    JsonOutputParser,\n",
        ")\n",
        "\n",
        "\n",
        "# A function that operates on finalized inputs\n",
        "# rather than on an input_stream\n",
        "def _extract_country_names(inputs):\n",
        "    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n",
        "    if not isinstance(inputs, dict):\n",
        "        return \"\"\n",
        "\n",
        "    if \"countries\" not in inputs:\n",
        "        return \"\"\n",
        "\n",
        "    countries = inputs[\"countries\"]\n",
        "\n",
        "    if not isinstance(countries, list):\n",
        "        return \"\"\n",
        "\n",
        "    country_names = [\n",
        "        country.get(\"name\") for country in countries if isinstance(country, dict)\n",
        "    ]\n",
        "    return country_names\n",
        "\n",
        "\n",
        "chain = model | JsonOutputParser() | _extract_country_names\n",
        "\n",
        "async for text in chain.astream(\n",
        "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'\n",
        "):\n",
        "    print(text, end=\"|\", flush=True)"
      ],
      "metadata": {
        "id": "P3panp-uKgi9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}