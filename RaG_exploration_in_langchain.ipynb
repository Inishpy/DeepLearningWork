{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Inishpy/DeepLearningWork/blob/main/RaG_exploration_in_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz_BArJHf4Mm"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install llama-cpp-python\n",
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50dtoA30fj0B"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"Avagin Updates.pdf\")\n",
        "pages = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "model = hf_hub_download(repo_id=\"TheBloke/phi-2-dpo-GGUF\", filename=\"phi-2-dpo.Q5_K_S.gguf\")"
      ],
      "metadata": {
        "id": "NjlRb8so4W34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUzCiRwQgvyA"
      },
      "outputs": [],
      "source": [
        "model = \"./phi-2-dpo.Q5_K_S.gguf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg1qy0pBg46B"
      },
      "outputs": [],
      "source": [
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crbzoFf0hB1a"
      },
      "outputs": [],
      "source": [
        "llm = LlamaCpp(\n",
        "    model_path=model,\n",
        "    temperature=0.75,\n",
        "    max_tokens=50,\n",
        "    top_p=1,\n",
        "\n",
        "    verbose=True,\n",
        "      n_ctx=2000,  # Verbose is required to pass to the callback manager\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVGud8YLhzcd"
      },
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "\n",
        "embeddings = OllamaEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEpBzeS4iaGu"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install pypdf2\n",
        "!pip install InstructorEmbedding==1.0.1\n",
        "!pip install sentence-transformers==2.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJ3Xfl2A2x64"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain_community.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-74l8sJU2x65"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_pdf_text(pdf):\n",
        "    text = \"\"\n",
        "\n",
        "    pdf_reader = PdfReader(pdf)\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def get_vectorstore(text_chunks):\n",
        "    #embeddings = OpenAIEmbeddings()\n",
        "    embeddings = HuggingFaceInstructEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9jB-luy2x67",
        "outputId": "df7baa91-5abd-4e9f-9559-73ca96f84ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "raw_text = get_pdf_text(\"Avagin Updates.pdf\")\n",
        "\n",
        "text_chunks = get_text_chunks(raw_text)\n",
        "\n",
        "vectorstore1 = get_vectorstore(text_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_LydYrwjGvv"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "documents = text_splitter.split_documents(pages)\n",
        "vectorstore2 = FAISS.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C99KbRQC2x69"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\"\"\")\n",
        "\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHkjzotN2x6-"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "retriever = vectorstore2.as_retriever()\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aaJLYV22x6_"
      },
      "outputs": [],
      "source": [
        "\n",
        "#this is how it works if we dont use chain\n",
        "docs = retriever.get_relevant_documents(\"Next weeks task?\")\n",
        "\n",
        "context = \"\"\n",
        "for doc in docs:\n",
        "    #print(doc)\n",
        "    context += doc.page_content + \"\\n\\n\"\n",
        "\n",
        "augmented_user_input = \"Context: \" + context + \"\\n\\nQuestion: \" + \"Next weeks task?\" + \"\\n\"\n",
        "\n",
        "print(augmented_user_input)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "retriever = vectorstore1.as_retriever()\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
      ],
      "metadata": {
        "id": "kAY5iWyM9fsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49af8922-c19c-419b-911e-8d31cd314335",
        "id": "C6Pc64XX8go-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: Challenges  and Request:  \n",
            "• GPU  Requirement:  Unfortunately, the fine -tuning and RAG processes require \n",
            "significant computational resources, specifically GPUs, which are currently \n",
            "unavailable.  \n",
            "• Domain -Specific  LLMs:  We aim to create eight specialized LLMs, including \n",
            "the Finance LLM designated for SatoshiAI. However, GPU access is crucial to \n",
            "achieve this objective.  \n",
            "Urgent  Request:  \n",
            "To ensure continued progress and meet development goals, we urgently request \n",
            "access to one or more GPUs. This will enable us to:  \n",
            "• Fine-tune  LLMs:  Tailor the LLMs to specific domains, including the Finance \n",
            "LLM for SatoshiAI.  \n",
            "• Run RAG  Scripts:  Enhance the LLMs' performance using the Retrieval -\n",
            "Augmented Generation technique.  \n",
            "• Achieve  Domain -Specific  Goals:  Develop the eight planned LLMs, including \n",
            "the critical Finance LLM for SatoshiAI.  \n",
            " \n",
            "Android  Integration:  We understand the importance of Android integration and will continue to prioritize\n",
            "\n",
            "Avagin Updates  \n",
            "Progress:  \n",
            "• Completed  Roger  Script:  The core script, integrating voice recognition, text -\n",
            "to-speech, and a large language model (LLM), is now functional.  \n",
            "• Flask  Demo  App:  A user -friendly Flask -based application showcases the \n",
            "Roger script's capabilities.  \n",
            "• Fine-Tuning  and RAG  Scripts  Ready:  Both fine -tuning scripts and Retrieval -\n",
            "Augmented Generation (RAG) scripts are prepared for domain -specific LLM \n",
            "development.  \n",
            "• UI and Android  App Demo  Prepared:  A basic Android app with user \n",
            "interface is ready for integration with the LLM. Upon successful integration, \n",
            "we can deliver a functional Android app demo within the next week for client \n",
            "presentation. This is not just a demo app; we plan to leverage this pr ogress \n",
            "for the final application.  \n",
            "Challenges  and Request:  \n",
            "• GPU  Requirement:  Unfortunately, the fine -tuning and RAG processes require \n",
            "significant computational resources, specifically GPUs, which are currently \n",
            "unavailable.\n",
            "\n",
            "powerful GPU.  \n",
            "• Kivy  Library  Exploration:  While initial attempts with the Kivy library \n",
            "encountered errors, further investigation might yield solutions for building a \n",
            "mobile app.  \n",
            "• Flutter  Framework  Evaluation:  We are actively exploring the potential of \n",
            "Flutter's Langchain integration to enable LLM execution on mobile devices. \n",
            "Further investigation into this option is warranted.  \n",
            "Additional  Considerations:  \n",
            "• Resource -Dependent  App:  We acknowledge that the Flask app is resource -\n",
            "dependent and requires a screen recording for demonstration purposes.  \n",
            "• Final  App Integration:  We plan to leverage the current progress on the UI \n",
            "and Android app for the final application.  \n",
            "Next  Week's  Tasks:  \n",
            "• Continue  investigating  and working  on Android  integration:  We will \n",
            "actively explore the mentioned solutions and prioritize the most promising \n",
            "approach based on feasibility and resource availability.\n",
            "\n",
            "actively explore the mentioned solutions and prioritize the most promising \n",
            "approach based on feasibility and resource availability.  \n",
            "• Integrate  LLM with Android  app:  Upon receiving GPU access and identifying \n",
            "a suitable solution, we will prioritize integrating the LLM with the Android app to \n",
            "deliver the promised demo within the next week.  \n",
            "• Integrate  automation  and API call for UBER  ride booking:  We will begin \n",
            "integrating automation and API calls to facilitate ride booking with Uber \n",
            "directly through the Roger script.  \n",
            "• Future  Script  Upgrades:  Further enhancements are planned for the script, \n",
            "including streaming capabilities, bot memory, translation features, and \n",
            "image/video input functionalities.  Demo  for Client:  \n",
            "• Flask  App Demo:  While awaiting GPU access and Android integration, we \n",
            "can showcase the Roger script's functionality through a screen recording of \n",
            "the Flask app demo.  \n",
            "• Android  App Demo:  Upon successful LLM integration, we will provide a\n",
            "\n",
            "\n",
            "\n",
            "Question: Next weeks task?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "docs = retriever.get_relevant_documents(\"Next weeks task?\")\n",
        "\n",
        "context = \"\"\n",
        "for doc in docs:\n",
        "    #print(doc)\n",
        "    context += doc.page_content + \"\\n\\n\"\n",
        "\n",
        "augmented_user_input = \"Context: \" + context + \"\\n\\nQuestion: \" + \"Next weeks task?\" + \"\\n\"\n",
        "\n",
        "print(augmented_user_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlkL0GIP2x7A",
        "outputId": "3850938e-c2af-4939-81b3-6f8c889cae4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =     862.39 ms\n",
            "llama_print_timings:      sample time =      41.98 ms /    50 runs   (    0.84 ms per token,  1190.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =  115260.26 ms /   965 tokens (  119.44 ms per token,     8.37 tokens per second)\n",
            "llama_print_timings:        eval time =    9275.99 ms /    49 runs   (  189.31 ms per token,     5.28 tokens per second)\n",
            "llama_print_timings:       total time =  125234.68 ms /  1014 tokens\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Next weeks task?',\n",
              " 'context': [Document(page_content=\"Challenges  and Request:  \\n• GPU  Requirement:  Unfortunately, the fine -tuning and RAG processes require \\nsignificant computational resources, specifically GPUs, which are currently \\nunavailable.  \\n• Domain -Specific  LLMs:  We aim to create eight specialized LLMs, including \\nthe Finance LLM designated for SatoshiAI. However, GPU access is crucial to \\nachieve this objective.  \\nUrgent  Request:  \\nTo ensure continued progress and meet development goals, we urgently request \\naccess to one or more GPUs. This will enable us to:  \\n• Fine-tune  LLMs:  Tailor the LLMs to specific domains, including the Finance \\nLLM for SatoshiAI.  \\n• Run RAG  Scripts:  Enhance the LLMs' performance using the Retrieval -\\nAugmented Generation technique.  \\n• Achieve  Domain -Specific  Goals:  Develop the eight planned LLMs, including \\nthe critical Finance LLM for SatoshiAI.  \\n \\nAndroid  Integration:  We understand the importance of Android integration and will continue to prioritize\"),\n",
              "  Document(page_content=\"Avagin Updates  \\nProgress:  \\n• Completed  Roger  Script:  The core script, integrating voice recognition, text -\\nto-speech, and a large language model (LLM), is now functional.  \\n• Flask  Demo  App:  A user -friendly Flask -based application showcases the \\nRoger script's capabilities.  \\n• Fine-Tuning  and RAG  Scripts  Ready:  Both fine -tuning scripts and Retrieval -\\nAugmented Generation (RAG) scripts are prepared for domain -specific LLM \\ndevelopment.  \\n• UI and Android  App Demo  Prepared:  A basic Android app with user \\ninterface is ready for integration with the LLM. Upon successful integration, \\nwe can deliver a functional Android app demo within the next week for client \\npresentation. This is not just a demo app; we plan to leverage this pr ogress \\nfor the final application.  \\nChallenges  and Request:  \\n• GPU  Requirement:  Unfortunately, the fine -tuning and RAG processes require \\nsignificant computational resources, specifically GPUs, which are currently \\nunavailable.\"),\n",
              "  Document(page_content=\"powerful GPU.  \\n• Kivy  Library  Exploration:  While initial attempts with the Kivy library \\nencountered errors, further investigation might yield solutions for building a \\nmobile app.  \\n• Flutter  Framework  Evaluation:  We are actively exploring the potential of \\nFlutter's Langchain integration to enable LLM execution on mobile devices. \\nFurther investigation into this option is warranted.  \\nAdditional  Considerations:  \\n• Resource -Dependent  App:  We acknowledge that the Flask app is resource -\\ndependent and requires a screen recording for demonstration purposes.  \\n• Final  App Integration:  We plan to leverage the current progress on the UI \\nand Android app for the final application.  \\nNext  Week's  Tasks:  \\n• Continue  investigating  and working  on Android  integration:  We will \\nactively explore the mentioned solutions and prioritize the most promising \\napproach based on feasibility and resource availability.\"),\n",
              "  Document(page_content=\"actively explore the mentioned solutions and prioritize the most promising \\napproach based on feasibility and resource availability.  \\n• Integrate  LLM with Android  app:  Upon receiving GPU access and identifying \\na suitable solution, we will prioritize integrating the LLM with the Android app to \\ndeliver the promised demo within the next week.  \\n• Integrate  automation  and API call for UBER  ride booking:  We will begin \\nintegrating automation and API calls to facilitate ride booking with Uber \\ndirectly through the Roger script.  \\n• Future  Script  Upgrades:  Further enhancements are planned for the script, \\nincluding streaming capabilities, bot memory, translation features, and \\nimage/video input functionalities.  Demo  for Client:  \\n• Flask  App Demo:  While awaiting GPU access and Android integration, we \\ncan showcase the Roger script's functionality through a screen recording of \\nthe Flask app demo.  \\n• Android  App Demo:  Upon successful LLM integration, we will provide a\")],\n",
              " 'answer': '\\nA) Continue investigating Android integration\\nB) Integrate LLM with Android app\\nC) Integrate automation and API call for UBER ride booking\\nD) Future script upgrades\\nE) Create a demo for clients\\nF) All of'}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_chain.invoke({\"input\": \"Next weeks task?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZRaUzzA2x7B"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", \"You are a helpful AI assistant named Envie. You will reply to questions only based on the context that you are provided. If something is out of context, you will refrain from replying and politely decline to respond to the user.\"), (\"user\", \"{input}\")]\n",
        ")\n",
        "\n",
        "#llm = ChatNVIDIA(model=\"mixtral_8x7b\")\n",
        "\n",
        "chain = prompt_template | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVoeITZT2x7C"
      },
      "outputs": [],
      "source": [
        "for response in chain.stream({\"input\": augmented_user_input}):\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuQkJacU2x7C"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}